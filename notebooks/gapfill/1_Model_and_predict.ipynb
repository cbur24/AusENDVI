{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gapfilling NDVI/LST with machine learning derived synthetic datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "Import Python packages that are used for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from joblib import dump, load\n",
    "from scipy import stats\n",
    "import geopandas as gpd\n",
    "from pprint import pprint\n",
    "from odc.geo.xr import assign_crs\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "import lightgbm as lgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import shap\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "import sys\n",
    "sys.path.append('/g/data/os22/chad_tmp/dea-notebooks/Tools/')\n",
    "from dea_tools.classification import predict_xr, HiddenPrints\n",
    "from dea_tools.spatial import xr_rasterize\n",
    "\n",
    "sys.path.append('/g/data/os22/chad_tmp/AusEFlux/src/')\n",
    "from _utils import round_coords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Using ggplot styles in this notebook\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_var='NDVI'\n",
    "feat = 'CLIM'\n",
    "n_samples = 30000\n",
    "n_val = 5000\n",
    "name = 'desert' #nondesert\n",
    "base = '/g/data/os22/chad_tmp/AusENDVI/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble datasets for training and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_var == 'NDVI':\n",
    "    datasets = [\n",
    "         f'{model_var}_harmonization/LGBM/{model_var}_{feat}_LGBM_5km_monthly_1982_2022_wGaps.nc',\n",
    "        '5km/MOY_5km_monthly_1982_2022.nc',\n",
    "        '5km/rain_5km_monthly_1981_2022.nc',\n",
    "        '5km/rain_cml3_5km_monthly_1982_2022.nc',\n",
    "        '5km/rain_cml6_5km_monthly_1982_2022.nc',\n",
    "        '5km/rain_anom_5km_monthly_1981_2022.nc',\n",
    "        '5km/rain_cml3_anom_5km_monthly_1982_2022.nc',\n",
    "        '5km/rain_cml6_anom_5km_monthly_1982_2022.nc',\n",
    "        '5km/vpd_5km_monthly_1982_2022.nc',\n",
    "        '5km/srad_5km_monthly_1982_2022.nc',\n",
    "        '5km/srad_anom_5km_monthly_1982_2022.nc',\n",
    "        '5km/tavg_5km_monthly_1982_2022.nc',\n",
    "        '5km/tavg_anom_5km_monthly_1982_2022.nc',\n",
    "        # '5km/Elevation_5km_monthly_1982_2022.nc',\n",
    "        '5km/CO2_5km_monthly_1982_2022.nc',\n",
    "        '5km/WCF_5km_monthly_1982_2022.nc',\n",
    "        # '5km/Aspect_5km_monthly_1982_2022.nc'\n",
    "               ]\n",
    "\n",
    "if model_var == 'LST':\n",
    "    datasets = [\n",
    "         f'{model_var}_harmonization/LGBM/{model_var}_{feat}_LGBM_5km_monthly_1982_2022_wGaps.nc',\n",
    "        '5km/MOY_5km_monthly_1982_2022.nc',\n",
    "        '5km/vpd_5km_monthly_1982_2022.nc',\n",
    "        '5km/srad_5km_monthly_1982_2022.nc',\n",
    "        '5km/srad_anom_5km_monthly_1982_2022.nc',\n",
    "        '5km/tavg_5km_monthly_1982_2022.nc',\n",
    "        '5km/tavg_anom_5km_monthly_1982_2022.nc',\n",
    "        '5km/rain_5km_monthly_1981_2022.nc',\n",
    "        '5km/rain_anom_5km_monthly_1981_2022.nc',\n",
    "        # '5km/Elevation_5km_monthly_1982_2022.nc',\n",
    "        '5km/WCF_5km_monthly_1982_2022.nc',\n",
    "        # '5km/Aspect_5km_monthly_1982_2022.nc'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss = []\n",
    "for d in datasets:\n",
    "    xx = xr.open_dataset(base+d).sel(time=slice('1982','2022'))\n",
    "    xx = assign_crs(xx, crs ='epsg:4326')\n",
    "    xx = round_coords(xx)\n",
    "    xx = xx.drop('spatial_ref')\n",
    "    dss.append(xx)\n",
    "\n",
    "ds = xr.merge(dss)\n",
    "ds = assign_crs(ds, crs ='epsg:4326')\n",
    "# ds = ds.sel(time=slice('2001','2022'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add modis summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_modis = ds[model_var].median('time')\n",
    "mean_modis = mean_modis.expand_dims(time=ds.time)\n",
    "ds[model_var+'_50'] = mean_modis\n",
    "\n",
    "min_modis = ds[model_var].quantile(0.05, dim='time').drop('quantile')\n",
    "min_modis = min_modis.expand_dims(time=ds.time)\n",
    "ds[model_var+'_05'] = min_modis\n",
    "\n",
    "max_modis = ds[model_var].quantile(0.95, dim='time').drop('quantile')\n",
    "max_modis = max_modis.expand_dims(time=ds.time)\n",
    "ds[model_var+'_95'] = max_modis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add derivative of rainfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if model_var == 'NDVI':\n",
    "#     derived = ds['rain'].differentiate(coord='time', datetime_unit='D')\n",
    "#     ds['rain_gradient'] = derived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mask for desert/nondesert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('/g/data/os22/chad_tmp/AusENDVI/data/bioclimatic_regions.geojson')\n",
    "\n",
    "desert = gdf[5:6] # grab desert\n",
    "mask = xr_rasterize(desert, ds)\n",
    "mask = round_coords(mask)\n",
    "\n",
    "if name=='nondesert':\n",
    "    mask = ~mask\n",
    "\n",
    "mask.plot(size=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.where(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & testing data: equal random sampling of bioclimatic regions\n",
    "\n",
    "Can skip to importing TD if already run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to save results \n",
    "results = []\n",
    "for index, row in gdf.iterrows():\n",
    "    \n",
    "    if name=='desert':\n",
    "        n=int(n_samples/1)\n",
    "        if (row['region_name']!='Desert'):\n",
    "            continue\n",
    "\n",
    "    if name=='nondesert':\n",
    "        n = int(n_samples/5)\n",
    "\n",
    "    if (name=='nondesert') & (row['region_name']=='Desert'):\n",
    "        continue\n",
    "    \n",
    "    print(row['region_name'], n)\n",
    "\n",
    "    # Generate a polygon mask to keep only data within the polygon\n",
    "    mask = xr_rasterize(gdf.iloc[[index]], ds[model_var])\n",
    "    mask = round_coords(mask)\n",
    "    \n",
    "    # Mask dataset to set pixels outside the polygon to `NaN`\n",
    "    dss = ds.where(mask)\n",
    "\n",
    "    #sample equivalent num of samples per region\n",
    "    df = dss.to_dataframe().dropna().sample(n=n, random_state=0).reset_index()\n",
    "    \n",
    "    # Append results to a dictionary using the attribute\n",
    "    # column as an key\n",
    "    results.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(results).reset_index(drop=True)\n",
    "df['year'] = pd.DatetimeIndex(df['time']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['time','spatial_ref'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = df.sample(n=n_val, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(validation.index)\n",
    "print(len(df), 'training samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the location of the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(\n",
    "    df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "gdf_val = gpd.GeoDataFrame(\n",
    "    validation, geometry=gpd.points_from_xy(validation.longitude, validation.latitude), crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "gdf = gdf.drop(['longitude'], axis=1)\n",
    "gdf_val = gdf_val.drop(['longitude'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_val.plot(column='year', cmap='inferno', markersize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_file(f'{base}synthetic/{model_var}/{model_var}_{name}_training_data.geojson')\n",
    "gdf_val.to_file(f'{base}synthetic/{model_var}/{model_var}_{name}_validation_data.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import training data if skipping above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf = gpd.read_file(f'{base}synthetic/{model_var}/{model_var}_{name}_training_data.geojson')\n",
    "# gdf_val = gpd.read_file(f'{base}synthetic/{model_var}/{model_var}_{name}_training_data.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(gdf.drop(columns='geometry', axis=1))\n",
    "validation = pd.DataFrame(gdf_val.drop(columns='geometry', axis=1))\n",
    "\n",
    "df = df.drop(['year'], axis=1)\n",
    "validation = validation.drop(['year'], axis=1)\n",
    "\n",
    "y = df[model_var]\n",
    "x = df.drop([model_var], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model using nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid using distributions\n",
    "param_grid = {\n",
    "    'num_leaves': stats.randint(5,50),\n",
    "    'min_child_samples':stats.randint(10,30),\n",
    "    'boosting_type': ['gbdt', 'dart'],\n",
    "    'max_depth': stats.randint(5,25),\n",
    "    'n_estimators': [300, 400, 500],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_cv = KFold(n_splits=5, shuffle=True,\n",
    "                   random_state=0)\n",
    "\n",
    "# lists to store results of CV testing\n",
    "acc = []\n",
    "rmse=[]\n",
    "r2=[]\n",
    "\n",
    "i = 1\n",
    "for train_index, test_index in outer_cv.split(x, y):\n",
    "    print(f\"Working on {i}/5 outer CV split\", end='\\r')\n",
    "    model = LGBMRegressor(random_state=1,\n",
    "                          verbose=-1,\n",
    "                          # n_jobs=-1\n",
    "                          )\n",
    "\n",
    "    # index training, testing\n",
    "    X_tr, X_tt = x.iloc[train_index, :], x.iloc[test_index, :]\n",
    "    y_tr, y_tt = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    #simple random split on inner fold\n",
    "    inner_cv = KFold(n_splits=3,\n",
    "                     shuffle=True,\n",
    "                     random_state=0)\n",
    "    \n",
    "    clf = RandomizedSearchCV(\n",
    "                   model,\n",
    "                   param_grid,\n",
    "                   verbose=0,\n",
    "                   n_iter=50,\n",
    "                   # n_jobs=-1,\n",
    "                   cv=inner_cv.split(X_tr, y_tr)\n",
    "                  )\n",
    "    \n",
    "    #prevents extensive print statements\n",
    "    clf.fit(X_tr, y_tr, callbacks=None)\n",
    "    \n",
    "    # predict using the best model\n",
    "    best_model = clf.best_estimator_\n",
    "    pred = best_model.predict(X_tt)\n",
    "\n",
    "    # evaluate model w/ multiple metrics\n",
    "    # r2\n",
    "    r2_ = r2_score(y_tt, pred)\n",
    "    r2.append(r2_)\n",
    "    # MAE\n",
    "    ac = mean_absolute_error(y_tt, pred)\n",
    "    acc.append(ac)\n",
    "    # RMSE\n",
    "    rmse_ = np.sqrt(mean_squared_error(y_tt, pred))\n",
    "    rmse.append(rmse_)\n",
    "    \n",
    "    #1:1 plots for each fold (save to csv so we can make a plot later on)\n",
    "    df = pd.DataFrame({'Test':y_tt, 'Pred':pred}).reset_index(drop=True)\n",
    "\n",
    "    df.to_csv(f'{base}synthetic/{model_var}/cross_val/{i}_{model_var}_{name}.csv')\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a single 1:1 plot out of the folds \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffs=[]\n",
    "for i in range(1,5+1):\n",
    "    df = pd.read_csv(f'{base}synthetic/{model_var}/cross_val/{i}_{model_var}_{name}.csv',\n",
    "                     usecols=['Test', 'Pred'])\n",
    "    dffs.append(df)\n",
    "\n",
    "cross_df = pd.concat(dffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1, figsize=(6,6))\n",
    "\n",
    "xy = np.vstack([cross_df['Test'],cross_df['Pred']])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "sb.scatterplot(data=cross_df, x='Test',y='Pred',cmap='magma', c=z, s=50, lw=1, alpha=0.5, ax=ax)\n",
    "sb.regplot(data=cross_df, x='Test',y='Pred', scatter=False, color='darkblue', ax=ax)\n",
    "sb.regplot(data=cross_df, x='Test',y='Test', color='black', scatter=False, line_kws={'linestyle':'dashed'}, ax=ax);\n",
    "\n",
    "plt.xlabel('Observation '+model_var, fontsize=16)\n",
    "plt.ylabel('Prediction '+model_var, fontsize=16)\n",
    "ax.text(.05, .95, 'r\\N{SUPERSCRIPT TWO}={:.2f}'.format(np.mean(r2)),\n",
    "            transform=ax.transAxes, fontsize=16)\n",
    "ax.text(.05, .9, 'MAE={:.2g}'.format(np.mean(acc)),\n",
    "            transform=ax.transAxes, fontsize=16)\n",
    "\n",
    "if model_var=='NDVI':\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=16)\n",
    "ax.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(f'{base}synthetic/{model_var}/cross_val/cross_val_{model_var}_{name}.png',\n",
    "            bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize model using all training data\n",
    "\n",
    "Using a randomized strategy so we can search through more variables, with 500 iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_cv = KFold(n_splits=5, shuffle=True,\n",
    "                   random_state=0)\n",
    "\n",
    "clf = RandomizedSearchCV(LGBMRegressor(verbose=-1),\n",
    "                   param_grid,\n",
    "                   verbose=1,\n",
    "                   n_iter=250,\n",
    "                   # n_jobs=-1,\n",
    "                   cv=outer_cv\n",
    "                  )\n",
    "\n",
    "clf.fit(x, y, callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The most accurate combination of tested parameters is: \")\n",
    "pprint(clf.best_params_)\n",
    "print('\\n')\n",
    "print(\"The best score using these parameters is: \")\n",
    "print(round(clf.best_score_, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit on all data using best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMRegressor(**clf.best_params_)\n",
    "\n",
    "model.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with independent validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = validation[model_var]\n",
    "x_val = validation.drop([model_var], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_val)\n",
    "\n",
    "r2 = r2_score(y_val, pred)\n",
    "ac = mean_absolute_error(y_val, pred)\n",
    "df_val = pd.DataFrame({'Test':y_val, 'Pred':pred}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1, figsize=(6,6))\n",
    "\n",
    "xy = np.vstack([df_val['Test'],df_val['Pred']])\n",
    "z = gaussian_kde(xy)(xy)\n",
    "\n",
    "sb.scatterplot(data=df_val, x='Test',y='Pred',cmap='magma',c=z, s=50, lw=1, alpha=0.5, ax=ax)\n",
    "sb.regplot(data=df_val, x='Test',y='Pred', scatter=False, color='darkblue', ax=ax)\n",
    "sb.regplot(data=df_val, x='Test',y='Test', color='black', scatter=False, line_kws={'linestyle':'dashed'}, ax=ax);\n",
    "\n",
    "plt.xlabel('Observation '+model_var, fontsize=16)\n",
    "plt.ylabel('Prediction '+model_var, fontsize=16)\n",
    "ax.text(.05, .95, 'r\\N{SUPERSCRIPT TWO}={:.2f}'.format(r2),\n",
    "            transform=ax.transAxes, fontsize=16)\n",
    "ax.text(.05, .9, 'MAE={:.2g}'.format(ac),\n",
    "            transform=ax.transAxes, fontsize=16)\n",
    "\n",
    "if model_var=='NDVI':\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xlim(0, 1)\n",
    "\n",
    "ax.tick_params(axis='x', labelsize=16)\n",
    "ax.tick_params(axis='y', labelsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(f'{base}synthetic/{model_var}/cross_val/validation_{model_var}_{name}.png',\n",
    "            bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(model, '/g/data/os22/chad_tmp/AusENDVI/results/models/gapfill/gapfill_'+model_var+'_'+name+'_LGBM.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load('/g/data/os22/chad_tmp/climate-carbon-interactions/results/models/gapfill/gapfill_'+model_var+'_'+name+'_LGBM.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine feature importance using SHAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain the model's predictions using SHAP\n",
    "explainer = shap.Explainer(model)\n",
    "shap_values = explainer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals= np.abs(shap_values.values).mean(0)\n",
    "feature_importance = pd.DataFrame(list(zip(x.columns, vals)), columns=['col_name','feature_importance_vals'])\n",
    "feature_importance.sort_values(by=['feature_importance_vals'],ascending=False,inplace=True)\n",
    "feature_importance['col_name'] = feature_importance['col_name'].str.removesuffix(\"_RS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(5,7))\n",
    "shap.plots.violin(shap_values, max_display=10, show=False, plot_type=\"layered_violin\")\n",
    "ax = plt.gca() \n",
    "plt.gcf().axes[-1].set_aspect('auto')\n",
    "plt.gcf().axes[-1].set_box_aspect(15)\n",
    "# plt.gcf().axes[-1].set_box_aspect(100)\n",
    "ax.tick_params(axis='x', labelsize=16)\n",
    "ax.tick_params(axis='y', labelsize=16)\n",
    "if name == 'desert':\n",
    "    ax.set_xlabel(model_var+ ' \"Desert\" '+'SHAP Value', fontsize=16)\n",
    "    ax.set_xlim(-0.05, 0.05)\n",
    "\n",
    "if name == 'nondesert':\n",
    "    ax.set_xlabel(model_var+ ' \"NonDesert\" '+'SHAP Value', fontsize=16)\n",
    "    ax.set_xlim(-0.1, 0.1)\n",
    "\n",
    "fig.savefig(f'{base}synthetic/{model_var}/cross_val/feature_importance_{model_var}_{name}.png',\n",
    "            bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make gridded predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacube.utils.dask import start_local_dask\n",
    "\n",
    "client = start_local_dask(mem_safety_margin='2Gb')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load('/g/data/os22/chad_tmp/climate-carbon-interactions/results/models/gapfill/gapfill_'+model_var+'_LGBM.joblib').set_params(n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add lat as a variable\n",
    "\n",
    "Plus ensure order of the variables is correct for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = ds.latitude\n",
    "lat = lat.expand_dims(time=ds.time, longitude=ds.longitude)\n",
    "lat = lat.transpose('time', 'latitude', 'longitude')\n",
    "ds['latitude_data'] = lat\n",
    "\n",
    "# lon = ds.longitude\n",
    "# lon = lon.expand_dims(time=ds.time, latitude=ds.latitude)\n",
    "# lon = lon.transpose('time', 'latitude', 'longitude')\n",
    "# ds['longitude_gridded'] = lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['NDVI_50'].mean(['x','y']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ds.data_vars)[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(ds.data_vars)[1:-1]\n",
    "# columns.insert(0, 'longitude_gridded')\n",
    "columns.insert(0, 'latitude_data')\n",
    "ds = ds[columns]\n",
    "ds = ds.rename({'latitude':'y', 'longitude':'x'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-create mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('/g/data/os22/chad_tmp/AusENDVI/data/bioclimatic_regions.geojson')\n",
    "\n",
    "desert = gdf[5:6] # grab desert\n",
    "mask = xr_rasterize(desert, ds)\n",
    "mask = round_coords(mask)\n",
    "\n",
    "if name=='nondesert':\n",
    "    mask = (~mask.astype(bool))\n",
    "\n",
    "# also create a land mask\n",
    "land_mask = ~np.isnan(ds.WCF.mean('time'))\n",
    "\n",
    "mask.plot(size=3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "i=0\n",
    "for i in range(0, len(ds.time)):\n",
    "    print(\" {:03}/{:03}\\r\".format(i + 1, len(range(0, len(ds.time)))), end=\"\")\n",
    "    with HiddenPrints():\n",
    "        predicted = predict_xr(model,\n",
    "                            ds.isel(time=i).drop('time'),\n",
    "                            proba=False,\n",
    "                            clean=True,\n",
    "                            chunk_size=100000,\n",
    "                              ).compute()\n",
    "    \n",
    "    # predicted = predicted.Predictions.where(~mask.isel(time=i))\n",
    "    predicted = predicted.assign_coords(time=ds.isel(time=i).time).expand_dims(time=1)\n",
    "    results.append(predicted.astype('float32'))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = xr.concat(results, dim='time').sortby('time').rename({'Predictions':model_var})#.astype('float32')\n",
    "yy = yy.where(land_mask)\n",
    "yy = yy.where(mask.rename({'latitude':'y', 'longitude':'x'}))\n",
    "yy = yy.rename({'y':'latitude', 'x':'longitude'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy[model_var].mean(['longitude','latitude']).plot(figsize=(11,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy.to_netcdf(f'{base}synthetic/{model_var}/{model_var}_{feat}_{name}_synthetic_5km_monthly_1982_2022.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1619aa7c0a0d41f892bc55d700874aba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ef7c7b47129f4bb7be210a31c9c6748c",
       "style": "IPY_MODEL_6c45957fae8a4c9c90d0efa71fb7acd3",
       "value": "100%"
      }
     },
     "1b9dd35d726a487c9e03b50cadec204e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "27c2edfba16545549cbc2446f65308f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "43e325ac2f4b42659d9ae321464c5dd1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_9b47cb4d6d96465b8a2446bc022c085d",
       "max": 217,
       "style": "IPY_MODEL_27c2edfba16545549cbc2446f65308f9",
       "value": 217
      }
     },
     "5ce2388f84c64f1eb830509fab75e200": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_1619aa7c0a0d41f892bc55d700874aba",
        "IPY_MODEL_43e325ac2f4b42659d9ae321464c5dd1",
        "IPY_MODEL_e6391b33874b4787837251849ceb2b1c"
       ],
       "layout": "IPY_MODEL_cc67c362ea5f4f80bfa29df7189ad3e6"
      }
     },
     "6c45957fae8a4c9c90d0efa71fb7acd3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "91df15d5486440d6a2125e74e9befe48": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9b47cb4d6d96465b8a2446bc022c085d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "cc67c362ea5f4f80bfa29df7189ad3e6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e6391b33874b4787837251849ceb2b1c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_1b9dd35d726a487c9e03b50cadec204e",
       "style": "IPY_MODEL_91df15d5486440d6a2125e74e9befe48",
       "value": " 217/217 [01:53&lt;00:00,  2.18it/s]"
      }
     },
     "ef7c7b47129f4bb7be210a31c9c6748c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
